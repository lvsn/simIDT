import gorilla
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json
import time

import torch
import torchvision.transforms as transforms
import cv2
import pandas as pd

from pem.model.pose_estimation_model import Net

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))

def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation")
    # pem
    parser.add_argument("--gpus",
                        type=str,
                        default="0",
                        help="path to pretrain model")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # input
    parser.add_argument("--output_dir", nargs="?", help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", help="Path to CAD(mm)")
    parser.add_argument("--data_path", nargs="?", help="Path to RGB image")
    parser.add_argument("--seg_path", nargs="?", help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    cfg = gorilla.Config.fromfile(args.config)
    cfg.exp_name = exp_name
    cfg.gpus     = args.gpus
    cfg.model_name = args.model
    cfg.log_dir  = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.data_path = args.data_path
    cfg.seg_path = args.seg_path

    cfg.det_score_thresh = args.det_score_thresh
    gorilla.utils.set_cuda_visible_devices(gpu_ids = cfg.gpus)

    return  cfg

from pem.utils.data_utils import (
    load_im,
    get_bbox,
    get_point_cloud_from_depth,
    get_resize_rgb_choose,
    compute_error
)
from pem.utils.draw_utils import draw_detections
import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])
comp_mean = np.array([0, 0, 0]).astype(np.float32)
comp_std = np.array([0, 0, 0]).astype(np.float32)

def gt_ref_2_pem_ref(pose):
  pose[1,:] = -pose[1,:]
  pose[2,:] = -pose[2,:]
  return pose

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat

def _get_template(path, cfg, tem_index=1):
    rgb_path = os.path.join(path, 'rgb', '{:05d}.png'.format(int(tem_index)))
    mask_path = os.path.join(path, 'masks', '{:05d}.png'.format(int(tem_index)))
    xyz_path = os.path.join(path, 'xyz', '{:05d}.npy'.format(int(tem_index)))
    depth_path = os.path.join(path, 'depth', '{:05d}.png'.format(int(tem_index)))

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0 
    mask = load_im(mask_path).astype(np.uint8)
    if len(mask.shape) == 3:
        mask = mask[:,:,0]
    mask = mask == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    global comp_mean, comp_std
    comp_mean += np.mean(rgb[mask], axis=(0))/255
    comp_std += np.std(rgb[mask], axis=(0))/255

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz

def get_templates(path, cfg):
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).cuda())
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).cuda())
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).cuda())

    global comp_mean, comp_std
    comp_mean /= n_template_view
    comp_std /= n_template_view
    # print('mean and std of templates: ')
    # print(comp_mean)
    # print(comp_std)
    return all_tem, all_tem_pts, all_tem_choose

def get_test_data(i, data_path, seq_path, model_points, cfg):
    cam_path = os.path.join(data_path, 'camera.json')
    cam_info = json.load(open(cam_path))
    K = np.array(cam_info['cam_K']).reshape(3, 3)

    rgb_path = os.path.join(seq_path, 'rgb', '{:05d}.png'.format(i))

    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []
    all_images = []
    
    
    depth_path = rgb_path.replace('rgb', 'depth')
    mask_path = rgb_path.replace('rgb', 'masks')

    rgb = load_im(rgb_path).astype(np.uint8)
    depth = load_im(depth_path).astype(np.float32) * cam_info['depth_scale'] / 1000.0
    # depth = depth[1:-1]
    pts = get_point_cloud_from_depth(depth, K)
    mask = (load_im(mask_path) / 255).astype(np.uint8)

    image = rgb.copy()

    mask = np.logical_and(mask > 0, depth > 0)
    if np.sum(mask) > 32:
        bbox = get_bbox(mask)
        y1, y2, x1, x2 = bbox
    else:
        print('No object detected')
        return None, None
    mask = mask[y1:y2, x1:x2]
    choose = mask.astype(np.float32).flatten().nonzero()[0]

    # pts
    cloud = pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
    center = np.mean(cloud, axis=0)
    tmp_cloud = cloud - center[None, :]
    flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
    if np.sum(flag) < 4:
        print('No enough points in the object')
        return None, None
    choose = choose[flag]
    cloud = cloud[flag]

    if len(choose) <= cfg.n_sample_observed_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
    choose = choose[choose_idx]
    cloud = cloud[choose_idx]

    # rgb
    rgb = rgb.copy()[y1:y2, x1:x2, :][:,:,::-1]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))
    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

    all_rgb.append(torch.FloatTensor(rgb))
    all_cloud.append(torch.FloatTensor(cloud))
    all_rgb_choose.append(torch.IntTensor(rgb_choose).long())

    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).cuda()
    ret_dict['rgb'] = torch.stack(all_rgb).cuda()
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).cuda()
    ret_dict['score'] = torch.FloatTensor(all_score).cuda()

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    return ret_dict, image
        
if __name__ == "__main__":
    cfg = init()

    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # model
    print("=> creating model ...")
    # MODEL = importlib.import_module(cfg.model_name)
    model = Net(cfg.model)
    model = model.cuda()
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    gorilla.solver.load_checkpoint(model=model, filename=checkpoint)

    print("=> extracting templates ...")
    # tem_path = os.path.join(cfg.output_dir, 'templates')
    tem_path = os.path.join(cfg.cad_path, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("=> loading input data ...")
    cad_path_real = os.path.join(cfg.cad_path, [f for f in os.listdir(cfg.cad_path) if f.endswith('.ply') or f.endswith('.obj')][0])
    mesh : trimesh.Trimesh = trimesh.load(cad_path_real, force='mesh')
    print(mesh.scale)
    model_points = mesh.sample(cfg.test_dataset.n_sample_model_point).astype(np.float32)
    if mesh.scale > 1:
        print('apply scale')
        model_points = model_points / 1000.0
    radius = np.max(np.linalg.norm(model_points, axis=1))
    print(radius)


    rgb_path = os.path.join(cfg.data_path, 'rgb')
    n_images = len([f for f in os.listdir(rgb_path) if f.endswith('.png')])
    print(f'Path: {cfg.data_path}')
    print(rgb_path)
    print(f'Number of images: {n_images}')

    pose_f = os.path.join(cfg.data_path, 'poses.npy')
    print(pose_f)
    pose = np.load(pose_f)

    total_test_data_time = 0
    total_model_time = 0
    error_df = pd.DataFrame(columns=['Translation', 'Rotation'])
    pred_poses = []
    for i in range(n_images):
        print(f'Processing {i}th instance ...')
        t0 = time.time()

        input_data, image = get_test_data(i, cfg.data_path, cfg.data_path, model_points, cfg.test_dataset)
        ninstance = input_data['pts'].size(0)

        total_test_data_time += time.time() - t0
        t1 = time.time()

        print("=> running model ...")
        with torch.no_grad():
            input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
            input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
            out = model(input_data)

        total_model_time += time.time() - t1

        if 'pred_pose_score' in out.keys():
            # print('pred_pose_score')
            # print(out['pred_pose_score'])
            # print(out['score'])
            pose_scores = out['pred_pose_score'] # * out['score']
        else:
            pose_scores = out['score']
        pose_scores = pose_scores.detach().cpu().numpy()
        pred_rot = out['pred_R'].detach().cpu().numpy()
        pred_trans = out['pred_t'].detach().cpu().numpy() * 1000
        # print(pose_scores)

        # pose_f = '/gel/usr/chren50/source/my_BundleSDF/data/demo/interaction_hard/clock/poses.npy'
        my_pose = pose[i]
        rot = pred_rot[0]
        trans = pred_trans[0] / 1000
        pred_pose = np.eye(4)
        pred_pose[:3,:3] = rot
        pred_pose[:3, 3] = trans
        pred_poses.append(pred_pose)
        pred_pose = gt_ref_2_pem_ref(pred_pose)
        my_pose = my_pose.reshape(4,4)
        diff_t, diff_r = compute_error(pred_pose, my_pose)
        print(f'Translation error: {diff_t}')
        print(f'Rotation error: {diff_r}')
        print(f'Pose score: {pose_scores[0]}')
        error_df = pd.concat([error_df, pd.DataFrame([[diff_t[0], diff_r[0]]], columns=['Translation', 'Rotation'])])

        # os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
        # os.makedirs(f"{cfg.output_dir}/sam6d_results/vis", exist_ok=True)
        # print("=> visualizating ...")
        # rgb = image
        # save_path = "{}/sam6d_results/vis/{:05d}.png".format(cfg.output_dir, i)
        # K = input_data['K'].detach().cpu().numpy()
        # vis_rgb = visualize(rgb, pred_rot, pred_trans, model_points*1000, K, save_path)
        # vis_rgb.save(save_path)

    print(f'Mean test data time: {total_test_data_time/n_images}')
    print(f'Mean model time: {total_model_time/n_images}')
    print(1/(total_model_time/n_images))
    error_df.to_csv(f"{cfg.output_dir}/sam6d_results/error.csv", index=False)
    pred_poses = np.array(pred_poses)
    np.save(f"{cfg.output_dir}/sam6d_results/pred_poses.npy", pred_poses)
    print(f'Mean translation error: {error_df["Translation"].mean()*1000}')
    print(f'Mean rotation error: {error_df["Rotation"].mean()}')